{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_site = pd.read_csv(\"/work/tadesse/beichen/ForDRI/Data/New_Bowen_Ratio/Filtered_AmeriFlux_Sites.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = glob.glob(\"/work/tadesse/beichen/ForDRI/Data/All_AmeriFlex_Data/\"+\"*.csv\")\n",
    "data_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US-xTA  is under processing... ...\n",
      "US-Uaf  is under processing... ...\n",
      "US-SRM  is under processing... ...\n",
      "US-Cst  is under processing... ...\n",
      "US-Ton  is under processing... ...\n",
      "US-NR1  is under processing... ...\n",
      "US-SP1  is under processing... ...\n",
      "US-xJE  is under processing... ...\n",
      "US-xPU  is under processing... ...\n",
      "US-Rws  is under processing... ...\n",
      "US-MMS  is under processing... ...\n",
      "US-xUK  is under processing... ...\n",
      "US-Ho2  is under processing... ...\n",
      "US-xSE  is under processing... ...\n",
      "US-Ha2  is under processing... ...\n",
      "US-UMB  is under processing... ...\n",
      "US-MOz  is under processing... ...\n",
      "US-Bar  is under processing... ...\n",
      "US-Slt  is under processing... ...\n",
      "US-Vcm  is under processing... ...\n",
      "US-NC2  is under processing... ...\n",
      "US-Oho  is under processing... ...\n",
      "US-Me2  is under processing... ...\n",
      "US-SSH  is under processing... ...\n",
      "PR-xGU  is under processing... ...\n",
      "US-Akn  is under processing... ...\n",
      "US-xRN  is under processing... ...\n",
      "US-EA5  is under processing... ...\n",
      "US-xNQ  is under processing... ...\n",
      "US-xSC  is under processing... ...\n",
      "US-Wrc  is under processing... ...\n",
      "US-WCr  is under processing... ...\n",
      "US-GLE  is under processing... ...\n"
     ]
    }
   ],
   "source": [
    "# for each site in the site dataframe\n",
    "for idx, r in df_site.iterrows():\n",
    "    site_id= r.loc[\"Site ID\"]\n",
    "    for d in data_list:\n",
    "        if d.split(\"_\")[3]==site_id:\n",
    "            print(site_id,\" is under processing... ...\")\n",
    "            df = pd.read_csv(d, delimiter = \",\",skiprows=2)\n",
    "            df = df.replace([-9999], np.nan)\n",
    "            if ['LE'] in df.columns.values:\n",
    "                df_dropna = df.dropna(subset = ['H','LE'],axis = 0)\n",
    "                df_dropna_cleaned = df_dropna.loc[:,['TIMESTAMP_START', 'TIMESTAMP_END','LE','H']]\n",
    "                df_dropna_cleaned = df_dropna_cleaned.rename(columns={\"TIMESTAMP_START\": \"TIMESTAMP_START\", \"TIMESTAMP_END\": \"TIMESTAMP_END\", \"LE\": \"LE\",\n",
    "                                            \"H\": \"H\"})\n",
    "            elif ['LE_1_1_1'] in df.columns.values:\n",
    "                df_dropna = df.dropna(subset = ['H_1_1_1','LE_1_1_1'],axis = 0)\n",
    "                df_dropna_cleaned = df_dropna.loc[:,['TIMESTAMP_START', 'TIMESTAMP_END','LE_1_1_1','H_1_1_1']]\n",
    "                df_dropna_cleaned = df_dropna_cleaned.rename(columns={\"TIMESTAMP_START\": \"TIMESTAMP_START\", \"TIMESTAMP_END\": \"TIMESTAMP_END\", \"LE_1_1_1\": \"LE\",\n",
    "                                            \"H_1_1_1\": \"H\"})\n",
    "            else:\n",
    "                print(df.columns)\n",
    "            \n",
    "            # filter the data if its flux measurement is lower than 50\n",
    "            df_dropna_cleaned = df_dropna_cleaned.loc[(df_dropna_cleaned['LE']>50)&(df_dropna_cleaned['H']>50),:]\n",
    "            df_dropna_cleaned.to_csv(\"/work/tadesse/beichen/ForDRI/Data/New_Bowen_Ratio/Cleaned_Flux_Data/\"+site_id+\".csv\")\n",
    "            \n",
    "            del df,df_dropna,df_dropna_cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_data_list = glob.glob(\"/work/tadesse/beichen/ForDRI/Data/New_Bowen_Ratio/Cleaned_Flux_Data/\"+\"*.csv\")\n",
    "flux_data_list.sort()\n",
    "for flux in flux_data_list:\n",
    "    fname = os.path.basename(flux)\n",
    "    print(fname[:-4],\" is under processing... ...\")\n",
    "    df = pd.read_csv(flux,index_col=0)\n",
    "    df['TIMESTAMP_START'] = df['TIMESTAMP_START'].map(lambda x: int(datetime.strptime(str(x)[:8], \"%Y%m%d\").strftime(\"%Y%j\")[:4])*100+int(datetime.strptime(str(x)[:8], \"%Y%m%d\").strftime(\"%Y%j\")[-3:])//7+1)\n",
    "    start_year = df.TIMESTAMP_START.values[0]//100\n",
    "    end_year = df.TIMESTAMP_START.values[-1]//100\n",
    "    print(start_year,end_year)\n",
    "    weekly_index = []\n",
    "    for i in range(start_year,end_year+1):\n",
    "        for j in range(1,53):\n",
    "            if j<10:\n",
    "                j='0'+str(j)\n",
    "            else:\n",
    "                j=str(j)\n",
    "            out = int(str(i)+str(j))\n",
    "            weekly_index.append(out)\n",
    "    df_out = pd.DataFrame(np.nan,index = weekly_index, columns = [\"LE\",\"H\"])\n",
    "    df = df.drop(['TIMESTAMP_END'],axis = 1)\n",
    "    df = df.groupby('TIMESTAMP_START').sum()\n",
    "    for idx in df.index.values:\n",
    "        df_out.loc[idx,:]=df.loc[idx,:].values\n",
    "    df_out = df_out.iloc[:(end_year-start_year+1)*52,:]\n",
    "    df_out[\"Bowen_ratio\"] = df_out.loc[:,\"H\"].values/df_out.loc[:,\"LE\"].values\n",
    "    df_out[\"Log_Bowen_ratio\"] = np.log10(df_out.loc[:,\"Bowen_ratio\"].values)\n",
    "    reshaped_logbr = df_out.Log_Bowen_ratio.values.reshape(end_year-start_year+1,52)\n",
    "    mean=np.nanmean(reshaped_logbr, axis = 0)\n",
    "    std = np.nanstd(reshaped_logbr, axis = 0)\n",
    "    std_logbr = (mean - reshaped_logbr)/std\n",
    "    df_out[\"STD_Log_Bowen_ratio\"] = std_logbr.flatten()\n",
    "    df_out.to_csv(\"/work/tadesse/beichen/ForDRI/Data/New_Bowen_Ratio/Bowen_Ratio/\"+fname)\n",
    "    \n",
    "    del df_out, df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
